default:
  base_config:
    env_name: UGVStreetSearch
    unity_args:
      build_path:
        win32: D:\Unity\win-RL-Envs\RLEnvironments.exe
        linux: /data/linux-RL-Envs/RLEnvironments.x86_64
      no_graphics: false

    n_envs: 50
    max_step: 150000
    reset_on_iteration: false

  reset_config:
    force_reset: true
    bounding_box_random: 0
    semantic_segmentation_random: 0
    success_reward: 10
    failure_reward: -1
    failure_backward_reward: -1

  nn_config:
    rep:
      blur: 0
      brightness: 1
      ray_random: 0
      need_speed: true

  sac_config:
    write_summary_per_step: 1000 # Write summaries in TensorBoard every N steps
    save_model_per_step: 10000 # Save model every N steps

    n_step: 3
    burn_in_step: 30
    seq_encoder: RNN

    discrete_dqn_like: true # Whether using policy or only Q network if discrete is in action spaces

    use_rnd: true
    rnd_n_sample: 50

  oc_config:
    nn_config:
      rep:
        blur: 0
        brightness: 1
        ray_random: 0
        need_speed: true

atc:
  base_config:
    name: "atc_{time}"
  sac_config:
    siamese: ATC

byol:
  base_config:
    name: "byol_{time}"
  sac_config:
    siamese: BYOL

atc_q:
  base_config:
    name: "atc_q_{time}"
  sac_config:
    siamese: ATC
    siamese_use_q: true

byol_q:
  base_config:
    name: "byol_q_{time}"
  sac_config:
    siamese: BYOL
    siamese_use_q: true

atc_q_ada:
  base_config:
    name: "atc_q_ada_{time}"
  sac_config:
    siamese: ATC
    siamese_use_q: true
    siamese_use_adaptive: true

byol_q_ada:
  base_config:
    name: "byol_q_ada_{time}"
  sac_config:
    siamese: BYOL
    siamese_use_q: true
    siamese_use_adaptive: true
