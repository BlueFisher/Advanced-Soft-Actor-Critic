import importlib.util
import logging
import shutil
import time
from collections import defaultdict
from pathlib import Path

from .. import sac_main
from ..sac_main import Main
from ..utils import UnifiedElapsedTimer, format_global_step
from ..utils.enums import *
from .oc_agent import OC_MultiAgentsManager
from .option_selector_base import OptionSelectorBase

sac_main.MultiAgentsManager = OC_MultiAgentsManager


class OC_Main(Main):
    def __init__(self, root_dir, config_dir, args):
        """
        config_path: the directory of config file
        args: command arguments generated by argparse
        """
        self._logger = logging.getLogger('oc')

        self._profiler = UnifiedElapsedTimer(self._logger)

        config_abs_dir = self._init_config(root_dir, config_dir, args)

        self._init_env()
        self._init_oc(config_abs_dir)

        self._run()

    def _init_oc(self, config_abs_dir: Path):
        for n, mgr in self.ma_manager:
            # If nn models exists, load saved model, or copy a new one
            saved_nn_abs_path = mgr.model_abs_dir / 'saved_nn.py'
            if not self.alway_use_env_nn and saved_nn_abs_path.exists():
                spec = importlib.util.spec_from_file_location('nn', str(saved_nn_abs_path))
                self._logger.info(f'Loaded nn from existed {saved_nn_abs_path}')
            else:
                nn_abs_path = config_abs_dir / f'{mgr.config["sac_config"]["nn"]}.py'

                spec = importlib.util.spec_from_file_location('nn', str(nn_abs_path))
                self._logger.info(f'Loaded nn in env dir: {nn_abs_path}')
                if not self.alway_use_env_nn:
                    shutil.copyfile(nn_abs_path, saved_nn_abs_path)

            nn = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(nn)
            mgr.config['sac_config']['nn'] = nn

            mgr.set_rl(OptionSelectorBase(num_options=mgr.config['oc_config']['num_options'],
                                          option_burn_in_step=mgr.config['oc_config']['option_burn_in_step'],
                                          option_nn_config=mgr.config['oc_config']['nn_config'],

                                          obs_names=mgr.obs_names,
                                          obs_shapes=mgr.obs_shapes,
                                          d_action_sizes=mgr.d_action_sizes,
                                          c_action_size=mgr.c_action_size,
                                          model_abs_dir=mgr.model_abs_dir,
                                          device=self.device,
                                          ma_name=None if len(self.ma_manager) == 1 else n,
                                          train_mode=self.train_mode,
                                          last_ckpt=self.last_ckpt,

                                          nn_config=mgr.config['nn_config'],
                                          **mgr.config['sac_config'],

                                          replay_config=mgr.config['replay_config']))

    def _run(self):
        ma_obs_list = self.env.reset(reset_config=self.reset_config)

        self.ma_manager.pre_run({
            n: obs_list[0].shape[0] for n, obs_list in ma_obs_list.items()
        })
        self.ma_manager.set_obs_list(ma_obs_list)

        force_reset = False
        iteration = 0
        trained_steps = 0

        try:
            while iteration != self.base_config['max_iter']:
                if self.base_config['max_step'] != -1 and trained_steps >= self.base_config['max_step']:
                    break

                if self.base_config['reset_on_iteration'] \
                        or self.ma_manager.is_max_reached() \
                        or force_reset:
                    ma_obs_list = self.env.reset(reset_config=self.reset_config)
                    self.ma_manager.set_obs_list(ma_obs_list)
                    self.ma_manager.clear()

                    force_reset = False
                else:
                    self.ma_manager.reset()

                step = 0
                iter_time = time.time()

                while not self.ma_manager.is_done():
                    self.ma_manager.burn_in_padding()

                    with self._profiler('get_ma_action', repeat=10):
                        ma_d_action, ma_c_action = self.ma_manager.get_ma_action(disable_sample=self.disable_sample)
                    # ma_option_index = self.ma_manager.get_option()
                    # k = list(ma_option_index.keys())[0]
                    # self.env.send_option(int(ma_option_index[k][0]))

                    with self._profiler('env.step', repeat=10):
                        (ma_next_obs_list,
                         ma_reward,
                         ma_local_done,
                         ma_max_reached) = self.env.step(ma_d_action, ma_c_action)

                    if ma_next_obs_list is None:
                        force_reset = True

                        self._logger.warning('Step encounters error, episode ignored')
                        continue

                    for n, mgr in self.ma_manager:
                        if step == self.base_config['max_step_each_iter']:
                            ma_local_done[n] = [True] * len(mgr.agents)
                            ma_max_reached[n] = [True] * len(mgr.agents)

                    self.ma_manager.set_ma_env_step(ma_next_obs_list,
                                                    ma_reward,
                                                    ma_local_done,
                                                    ma_max_reached)

                    if self.train_mode:
                        with self._profiler('train', repeat=10):
                            trained_steps = self.ma_manager.train(trained_steps)

                    self.ma_manager.post_step(ma_next_obs_list, ma_local_done)

                    step += 1

                if self.train_mode:
                    self._log_episode_summaries()

                self._log_episode_info(iteration, time.time() - iter_time)

                p_model = self.model_abs_dir.joinpath('save_model')
                if self.train_mode and p_model.exists():
                    p_replay_buffer = self.model_abs_dir.joinpath('save_replay_buffer')
                    if p_replay_buffer.exists():
                        self.ma_manager.save_model(p_replay_buffer.exists())
                        p_replay_buffer.unlink()
                    else:
                        self.ma_manager.save_model(False)

                    p_model.unlink()

                iteration += 1

        finally:
            if self.train_mode:
                self.ma_manager.save_model()
            self.env.close()

            self._logger.info('Training terminated')

    def _log_episode_info(self, iteration, iter_time):
        for n, mgr in self.ma_manager:
            global_step = format_global_step(mgr.rl.get_global_step())
            rewards = [a.reward for a in mgr.agents]
            rewards = ", ".join([f"{i:6.1f}" for i in rewards])

            option_index_count = defaultdict(int)
            for oic in [a.option_index_count for a in mgr.agents]:
                for option_index, count in oic.items():
                    option_index_count[option_index] += count
            self._logger.info(', '.join([f'{k}: {v}' for k, v in option_index_count.items()]))

            max_step = max([a.steps for a in mgr.agents])
            self._logger.info(f'{n} {iteration}({global_step}), T {iter_time:.2f}s, S {max_step}, R {rewards}')
