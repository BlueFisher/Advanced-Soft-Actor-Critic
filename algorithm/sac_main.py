import importlib.util
import logging
import shutil
import time
from pathlib import Path
from typing import Dict, Set

import numpy as np

import algorithm.config_helper as config_helper

from .agent import MultiAgentsManager
from .sac_base import SAC_Base
from .utils import UnifiedElapsedTimer, format_global_step
from .utils.enums import *


class Main:
    train_mode = True
    render = False
    unity_run_in_editor = False

    ma_manager: MultiAgentsManager

    def __init__(self, root_dir, config_dir, args):
        """
        config_path: the directory of config file
        args: command arguments generated by argparse
        """
        self._logger = logging.getLogger('sac')

        self._profiler = UnifiedElapsedTimer(self._logger)

        config_abs_dir = self._init_config(root_dir, config_dir, args)

        self._init_env()
        self._init_sac(config_abs_dir)

        self._run()

    def _init_config(self, root_dir, config_dir, args):
        config_abs_dir = Path(root_dir).joinpath(config_dir)
        config_abs_path = config_abs_dir.joinpath('config.yaml')
        default_config_abs_path = Path(__file__).resolve().parent.joinpath('default_config.yaml')
        # Merge default_config.yaml and custom config.yaml
        config, ma_configs = config_helper.initialize_config_from_yaml(default_config_abs_path,
                                                                       config_abs_path,
                                                                       config_cat=args.config,
                                                                       override=args.override)

        # Initialize config from command line arguments
        self.train_mode = not args.run
        self.inference_ma_names: Set[str] = set(args.run_a)
        self.render = args.render
        self.unity_run_in_editor = args.editor
        self.unity_time_scale = args.timescale

        self.disable_sample = args.disable_sample
        self.alway_use_env_nn = args.use_env_nn
        self.device = args.device
        self.last_ckpt = args.ckpt

        if len(args.env_args) > 0:
            config['base_config']['env_args'] = args.env_args
        if args.port is not None:
            config['base_config']['unity_args']['port'] = args.port
        if args.envs is not None:
            config['base_config']['n_envs'] = args.envs
        if args.max_iter is not None:
            config['base_config']['max_iter'] = args.max_iter
        if args.name is not None:
            config['base_config']['name'] = args.name
        if args.nn is not None:
            config['sac_config']['nn'] = args.nn
            for ma_config in ma_configs.values():
                ma_config['sac_config']['nn'] = args.nn

        config['base_config']['name'] = config_helper.generate_base_name(config['base_config']['name'])

        # The absolute directory of a specific training
        model_abs_dir = Path(root_dir).joinpath('models',
                                                config['base_config']['env_name'],
                                                config['base_config']['name'])
        model_abs_dir.mkdir(parents=True, exist_ok=True)
        self.model_abs_dir = model_abs_dir

        if args.logger_in_file:
            config_helper.add_file_logger(model_abs_dir.joinpath(f'log.log'))

        if self.train_mode:
            config_helper.save_config(config, model_abs_dir, 'config.yaml')
        config_helper.display_config(config, self._logger)
        convert_config_to_enum(config['sac_config'])
        convert_config_to_enum(config['oc_config'])

        for n, ma_config in ma_configs.items():
            if self.train_mode and n not in self.inference_ma_names:
                config_helper.save_config(ma_config, model_abs_dir, f'config_{n.replace("?", "-")}.yaml')
            config_helper.display_config(ma_config, self._logger, n)
            convert_config_to_enum(ma_config['sac_config'])
            convert_config_to_enum(ma_config['oc_config'])

        self.base_config = config['base_config']
        self.reset_config = config['reset_config']
        self.config = config
        self.ma_configs = ma_configs

        return config_abs_dir

    def _init_env(self):
        if self.base_config['env_type'] == 'UNITY':
            from algorithm.env_wrapper.unity_wrapper import UnityWrapper

            if self.unity_run_in_editor:
                self.env = UnityWrapper(train_mode=self.train_mode,
                                        n_envs=self.base_config['n_envs'],
                                        max_n_envs_per_process=self.base_config['unity_args']['max_n_envs_per_process'],
                                        time_scale=self.unity_time_scale,
                                        env_args=self.base_config['env_args'])
            else:
                self.env = UnityWrapper(train_mode=self.train_mode,
                                        env_name=self.base_config['unity_args']['build_path'],
                                        n_envs=self.base_config['n_envs'],
                                        base_port=self.base_config['unity_args']['port'],
                                        max_n_envs_per_process=self.base_config['unity_args']['max_n_envs_per_process'],
                                        no_graphics=self.base_config['unity_args']['no_graphics'] and not self.render,
                                        force_vulkan=self.base_config['unity_args']['force_vulkan'],
                                        time_scale=self.unity_time_scale,
                                        scene=self.base_config['env_name'],
                                        env_args=self.base_config['env_args'])

        elif self.base_config['env_type'] == 'GYM':
            from algorithm.env_wrapper.gym_wrapper import GymWrapper

            self.env = GymWrapper(train_mode=self.train_mode,
                                  env_name=self.base_config['env_name'],
                                  env_args=self.base_config['env_args'],
                                  n_envs=self.base_config['n_envs'],
                                  render=self.render)

        elif self.base_config['env_type'] == 'OFFLINE':
            from algorithm.env_wrapper.offline_wrapper import OfflineWrapper

            self.env = OfflineWrapper(env_name=self.base_config['env_name'],
                                      env_args=self.base_config['env_args'],
                                      n_envs=self.base_config['n_envs'])

        elif self.base_config['env_type'] == 'TEST':
            from algorithm.env_wrapper.test_wrapper import TestWrapper

            self.env = TestWrapper(env_args=self.base_config['env_args'],
                                   n_envs=self.base_config['n_envs'])

        else:
            raise RuntimeError(f'Undefined Environment Type: {self.base_config["env_type"]}')

        ma_obs_names, ma_obs_shapes, ma_d_action_sizes, ma_c_action_size = self.env.init()
        self.ma_manager = MultiAgentsManager(ma_obs_names,
                                             ma_obs_shapes,
                                             ma_d_action_sizes,
                                             ma_c_action_size,
                                             self.inference_ma_names,
                                             self.model_abs_dir)
        for n, mgr in self.ma_manager:
            if n not in self.ma_configs:
                self._logger.warning(f'{n} not in ma_configs')
                mgr.set_config(self.config)
            else:
                mgr.set_config(self.ma_configs[n])

            self._logger.info(f'{n} observation shapes: {mgr.obs_shapes}')
            self._logger.info(f'{n} discrete action sizes: {mgr.d_action_sizes}')
            self._logger.info(f'{n} continuous action size: {mgr.c_action_size}')

        self._logger.info(f'{self.base_config["env_name"]} initialized')

    def _init_sac(self, config_abs_dir: Path):
        for n, mgr in self.ma_manager:
            # If nn models exists, load saved model, or copy a new one
            saved_nn_abs_path = mgr.model_abs_dir / 'saved_nn.py'
            if not self.alway_use_env_nn and saved_nn_abs_path.exists():
                spec = importlib.util.spec_from_file_location('nn', str(saved_nn_abs_path))
                self._logger.info(f'Loaded nn from existed {saved_nn_abs_path}')
            else:
                nn_abs_path = config_abs_dir / f'{mgr.config["sac_config"]["nn"]}.py'

                spec = importlib.util.spec_from_file_location('nn', str(nn_abs_path))
                self._logger.info(f'Loaded nn in env dir: {nn_abs_path}')
                if not self.alway_use_env_nn:
                    shutil.copyfile(nn_abs_path, saved_nn_abs_path)

            nn = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(nn)
            mgr.config['sac_config']['nn'] = nn

            mgr.set_rl(SAC_Base(obs_names=mgr.obs_names,
                                obs_shapes=mgr.obs_shapes,
                                d_action_sizes=mgr.d_action_sizes,
                                c_action_size=mgr.c_action_size,
                                model_abs_dir=mgr.model_abs_dir,
                                device=self.device,
                                ma_name=None if len(self.ma_manager) == 1 else n,
                                train_mode=self.train_mode and n not in self.inference_ma_names,
                                last_ckpt=self.last_ckpt,

                                nn_config=mgr.config['nn_config'],
                                **mgr.config['sac_config'],

                                replay_config=mgr.config['replay_config']))

    def _run(self):
        force_reset = False
        is_training = False  # Is current iteration training
        inference_iteration = 0  # The inference iteration count
        trained_steps = 0  # The steps that RL trained

        self.ma_manager.set_train_mode(False)  # The first iteration is inference

        try:
            while inference_iteration != self.base_config['max_iter']:
                if self.base_config['max_step'] != -1 and trained_steps >= self.base_config['max_step']:
                    break

                step = 0
                iter_time = time.time()

                if inference_iteration == 0 \
                        or self.base_config['reset_on_iteration'] \
                        or self.ma_manager.max_reached \
                        or force_reset:
                    self.ma_manager.reset()
                    ma_agent_ids, ma_obs_list = self.env.reset(reset_config=self.reset_config)
                    ma_d_action, ma_c_action = self.ma_manager.get_ma_action(
                        ma_agent_ids=ma_agent_ids,
                        ma_obs_list=ma_obs_list,
                        ma_last_reward={n: np.zeros(len(agent_ids), dtype=bool)
                                        for n, agent_ids in ma_agent_ids.items()},
                        disable_sample=self.disable_sample
                    )

                    force_reset = False
                else:
                    self.ma_manager.reset_and_continue()

                while not self.ma_manager.done:
                    with self._profiler('env.step', repeat=10):
                        (decision_step,
                         terminal_step,
                         all_envs_done) = self.env.step(ma_d_action, ma_c_action)
                        self._extra_step(ma_d_action, ma_c_action)

                    if decision_step is None:
                        force_reset = True

                        self._logger.error('Step encounters error, episode ignored')
                        break

                    with self._profiler('get_ma_action', repeat=10):
                        ma_d_action, ma_c_action = self.ma_manager.get_ma_action(
                            ma_agent_ids=decision_step.ma_agent_ids,
                            ma_obs_list=decision_step.ma_obs_list,
                            ma_last_reward=decision_step.ma_last_reward,
                            disable_sample=self.disable_sample
                        )

                    self.ma_manager.end_episode(
                        ma_agent_ids=terminal_step.ma_agent_ids,
                        ma_obs_list=terminal_step.ma_obs_list,
                        ma_last_reward=terminal_step.ma_last_reward,
                        ma_max_reached=terminal_step.ma_max_reached
                    )
                    if all_envs_done or step == self.base_config['max_step_each_iter']:
                        self.ma_manager.end_episode(
                            ma_agent_ids=decision_step.ma_agent_ids,
                            ma_obs_list=decision_step.ma_obs_list,
                            ma_last_reward=decision_step.ma_last_reward,
                            ma_max_reached={n: np.ones_like(agent_ids, dtype=bool)
                                            for n, agent_ids in decision_step.ma_agent_ids.items()}
                        )
                        self.ma_manager.force_end_all_episode()

                    if self.train_mode and is_training:
                        with self._profiler('train', repeat=10) as profiler:
                            next_trained_steps = self.ma_manager.train(trained_steps)
                            if next_trained_steps == trained_steps:
                                profiler.ignore()
                            trained_steps = next_trained_steps
                    elif self.train_mode and not is_training:
                        self.ma_manager.log_episode()

                    step += 1

                if self.train_mode and not is_training:
                    self._log_episode_summaries()

                if not is_training:
                    self._log_episode_info(inference_iteration, time.time() - iter_time)

                if self.train_mode:
                    is_training = not is_training
                    self.ma_manager.set_train_mode(is_training)

                self.ma_manager.reset_dead_agents()

                p_model = self.model_abs_dir.joinpath('save_model')
                if self.train_mode and p_model.exists():
                    p_replay_buffer = self.model_abs_dir.joinpath('save_replay_buffer')
                    if p_replay_buffer.exists():
                        self.ma_manager.save_model(p_replay_buffer.exists())
                        p_replay_buffer.unlink()
                    else:
                        self.ma_manager.save_model(False)

                    p_model.unlink()

                if not is_training:
                    inference_iteration += 1

        except KeyboardInterrupt:
            self._logger.warning('KeyboardInterrupt')

        finally:
            if self.train_mode:
                self.ma_manager.save_model()
            self.env.close()

            self._logger.info('Training terminated')

    def _extra_step(self,
                    ma_d_action: Dict[str, np.ndarray],
                    ma_c_action: Dict[str, np.ndarray]):
        pass

    def _log_episode_summaries(self):
        for n, mgr in self.ma_manager:
            if n in self.inference_ma_names or len(mgr.non_empty_agents) == 0:
                continue

            rewards = np.array([a.reward for a in mgr.non_empty_agents])
            mgr.rl.write_constant_summaries([
                {'tag': 'reward/mean', 'simple_value': rewards.mean()},
                {'tag': 'reward/max', 'simple_value': rewards.max()},
                {'tag': 'reward/min', 'simple_value': rewards.min()}
            ])
            mgr.rl.write_histogram_summaries([
                {'tag': 'reward', 'histogram': rewards}
            ])

            steps = np.array([a.steps for a in mgr.non_empty_agents])

            mgr.rl.write_constant_summaries([
                {'tag': 'metric/steps', 'simple_value': steps.mean()},
            ])

    def _log_episode_info(self, iteration, iter_time):
        for n, mgr in self.ma_manager:
            if len(mgr.non_empty_agents) == 0:
                continue
            global_step = format_global_step(mgr.rl.get_global_step())
            rewards = [a.reward for a in mgr.non_empty_agents]
            rewards = ", ".join([f"{i:6.1f}" for i in rewards])
            max_step = max([a.steps for a in mgr.non_empty_agents])
            self._logger.info(f'{n} {iteration}({global_step}), T {iter_time:.2f}s, S {max_step}, R {rewards}')
