import importlib.util
import logging
import shutil
import sys
import time
from pathlib import Path

import numpy as np

import algorithm.config_helper as config_helper

from .agent import MultiAgentsManager
from .sac_base import SAC_Base
from .utils import UnifiedElapsedTimer, format_global_step
from .utils.enums import *


class Main:
    train_mode = True
    render = False
    unity_run_in_editor = False

    def __init__(self, root_dir, config_dir, args):
        """
        config_path: the directory of config file
        args: command arguments generated by argparse
        """
        self._logger = logging.getLogger('sac')

        self._profiler = UnifiedElapsedTimer(self._logger)

        config_abs_dir = self._init_config(root_dir, config_dir, args)

        self._init_env()
        self._init_sac(config_abs_dir)

        self._run()

    def _init_config(self, root_dir, config_dir, args):
        config_abs_dir = Path(root_dir).joinpath(config_dir)
        config_abs_path = config_abs_dir.joinpath('config.yaml')
        default_config_abs_path = Path(__file__).resolve().parent.joinpath('default_config.yaml')
        # Merge default_config.yaml and custom config.yaml
        config, ma_configs = config_helper.initialize_config_from_yaml(default_config_abs_path,
                                                                       config_abs_path,
                                                                       args.config)

        # Initialize config from command line arguments
        self.train_mode = not args.run
        self.render = args.render
        self.unity_run_in_editor = args.editor

        self.disable_sample = args.disable_sample
        self.alway_use_env_nn = args.use_env_nn
        self.device = args.device
        self.last_ckpt = args.ckpt

        if args.env_args is not None:
            config['base_config']['env_args'] = args.env_args
        if args.port is not None:
            config['base_config']['unity_args']['port'] = args.port
        if args.envs is not None:
            config['base_config']['n_envs'] = args.envs
        if args.max_iter is not None:
            config['base_config']['max_iter'] = args.max_iter
        if args.name is not None:
            config['base_config']['name'] = args.name
        if args.nn is not None:
            config['sac_config']['nn'] = args.nn
            for ma_config in ma_configs.values():
                ma_config['sac_config']['nn'] = args.nn

        config['base_config']['name'] = config_helper.generate_base_name(config['base_config']['name'])

        # The absolute directory of a specific training
        model_abs_dir = Path(root_dir).joinpath('models',
                                                config['base_config']['env_name'],
                                                config['base_config']['name'])
        model_abs_dir.mkdir(parents=True, exist_ok=True)
        self.model_abs_dir = model_abs_dir

        if args.logger_in_file:
            config_helper.set_logger(model_abs_dir.joinpath(f'log.log'))

        if self.train_mode:
            config_helper.save_config(config, model_abs_dir, 'config.yaml')
        config_helper.display_config(config, self._logger)
        convert_config_to_enum(config['sac_config'])

        for n, ma_config in ma_configs.items():
            if self.train_mode:
                config_helper.save_config(ma_config, model_abs_dir, f'config_{n.replace("?", "-")}.yaml')
            config_helper.display_config(ma_config, self._logger, n)
            convert_config_to_enum(ma_config['sac_config'])

        self.base_config = config['base_config']
        self.reset_config = config['reset_config']
        self.config = config
        self.ma_configs = ma_configs

        return config_abs_dir

    def _init_env(self):
        if self.base_config['env_type'] == 'UNITY':
            from algorithm.env_wrapper.unity_wrapper import UnityWrapper

            if self.unity_run_in_editor:
                self.env = UnityWrapper(train_mode=self.train_mode,
                                        n_envs=self.base_config['n_envs'],
                                        group_aggregation=self.base_config['unity_args']['group_aggregation'])
            else:
                self.env = UnityWrapper(train_mode=self.train_mode,
                                        file_name=self.base_config['unity_args']['build_path'][sys.platform],
                                        base_port=self.base_config['unity_args']['port'],
                                        no_graphics=self.base_config['unity_args']['no_graphics'] and not self.render,
                                        scene=self.base_config['env_name'],
                                        additional_args=self.base_config['env_args'],
                                        n_envs=self.base_config['n_envs'],
                                        group_aggregation=self.base_config['unity_args']['group_aggregation'])

        elif self.base_config['env_type'] == 'GYM':
            from algorithm.env_wrapper.gym_wrapper import GymWrapper

            self.env = GymWrapper(train_mode=self.train_mode,
                                  env_name=self.base_config['env_name'],
                                  render=self.render,
                                  n_envs=self.base_config['n_envs'])

        elif self.base_config['env_type'] == 'DM_CONTROL':
            from algorithm.env_wrapper.dm_control_wrapper import \
                DMControlWrapper

            self.env = DMControlWrapper(train_mode=self.train_mode,
                                        env_name=self.base_config['env_name'],
                                        render=self.render,
                                        n_envs=self.base_config['n_envs'])

        elif self.base_config['env_type'] == 'TEST':
            from algorithm.env_wrapper.test_wrapper import TestWrapper

            self.env = TestWrapper(env_args=self.base_config['env_args'],
                                   n_envs=self.base_config['n_envs'])

        else:
            raise RuntimeError(f'Undefined Environment Type: {self.base_config["env_type"]}')

        ma_obs_names, ma_obs_shapes, ma_d_action_sizes, ma_c_action_size = self.env.init()
        self.ma_manager = MultiAgentsManager(ma_obs_names,
                                             ma_obs_shapes,
                                             ma_d_action_sizes,
                                             ma_c_action_size,
                                             self.model_abs_dir)
        for n, mgr in self.ma_manager:
            if n not in self.ma_configs:
                self._logger.warning(f'{n} not in ma_configs')
                mgr.set_config(self.config)
            else:
                mgr.set_config(self.ma_configs[n])

            self._logger.info(f'{n} observation shapes: {mgr.obs_shapes}')
            self._logger.info(f'{n} action shapes: {mgr.d_action_sizes}, {mgr.c_action_size}')

        self._logger.info(f'{self.base_config["env_name"]} initialized')

    def _init_sac(self, config_abs_dir: Path):
        for n, mgr in self.ma_manager:
            # If nn models exists, load saved model, or copy a new one
            saved_nn_abs_path = mgr.model_abs_dir / 'saved_nn.py'
            if not self.alway_use_env_nn and saved_nn_abs_path.exists():
                spec = importlib.util.spec_from_file_location('nn', str(saved_nn_abs_path))
                self._logger.info(f'Loaded nn from existed {saved_nn_abs_path}')
            else:
                nn_abs_path = config_abs_dir / f'{mgr.config["sac_config"]["nn"]}.py'

                spec = importlib.util.spec_from_file_location('nn', str(nn_abs_path))
                self._logger.info(f'Loaded nn in env dir: {nn_abs_path}')
                if not self.alway_use_env_nn:
                    shutil.copyfile(nn_abs_path, saved_nn_abs_path)

            nn = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(nn)
            mgr.config['sac_config']['nn'] = nn

            mgr.set_rl(SAC_Base(obs_names=mgr.obs_names,
                                obs_shapes=mgr.obs_shapes,
                                d_action_sizes=mgr.d_action_sizes,
                                c_action_size=mgr.c_action_size,
                                model_abs_dir=mgr.model_abs_dir,
                                device=self.device,
                                ma_name=None if len(self.ma_manager) == 1 else n,
                                train_mode=self.train_mode,
                                last_ckpt=self.last_ckpt,

                                nn_config=mgr.config['nn_config'],
                                **mgr.config['sac_config'],

                                replay_config=mgr.config['replay_config']))

    def _run(self):
        ma_obs_list = self.env.reset(reset_config=self.reset_config)

        self.ma_manager.pre_run({
            n: obs_list[0].shape[0] for n, obs_list in ma_obs_list.items()
        })
        self.ma_manager.set_obs_list(ma_obs_list)

        force_reset = False
        iteration = 0
        trained_steps = 0

        try:
            while iteration != self.base_config['max_iter']:
                if self.base_config['max_step'] != -1 and trained_steps >= self.base_config['max_step']:
                    break

                if self.base_config['reset_on_iteration'] \
                        or self.ma_manager.is_max_reached() \
                        or force_reset:
                    ma_obs_list = self.env.reset(reset_config=self.reset_config)
                    self.ma_manager.set_obs_list(ma_obs_list)
                    self.ma_manager.clear()

                    force_reset = False
                else:
                    self.ma_manager.reset()

                step = 0
                iter_time = time.time()

                while not self.ma_manager.is_done():
                    with self._profiler('burn_in_padding', repeat=10):
                        self.ma_manager.burn_in_padding()

                    with self._profiler('get_ma_action', repeat=10):
                        ma_d_action, ma_c_action = self.ma_manager.get_ma_action(disable_sample=self.disable_sample)
                        # ma_d_action, ma_c_action = self.ma_manager.get_test_ma_action()

                    with self._profiler('env.step', repeat=10):
                        (ma_next_obs_list,
                         ma_reward,
                         ma_local_done,
                         ma_max_reached) = self.env.step(ma_d_action, ma_c_action)

                    if ma_next_obs_list is None:
                        force_reset = True

                        self._logger.warning('Step encounters error, episode ignored')
                        continue

                    for n, mgr in self.ma_manager:
                        if step == self.base_config['max_step_each_iter']:
                            ma_local_done[n] = [True] * len(mgr.agents)
                            ma_max_reached[n] = [True] * len(mgr.agents)

                    self.ma_manager.set_ma_env_step(ma_next_obs_list,
                                                    ma_reward,
                                                    ma_local_done,
                                                    ma_max_reached)

                    if self.train_mode:
                        with self._profiler('train', repeat=10):
                            trained_steps = self.ma_manager.train(trained_steps)

                    with self._profiler('post_step', repeat=10):
                        self.ma_manager.post_step(ma_next_obs_list, ma_local_done)

                    step += 1

                if self.train_mode:
                    self._log_episode_summaries()

                self._log_episode_info(iteration, time.time() - iter_time)

                p_model = self.model_abs_dir.joinpath('save_model')
                if self.train_mode and p_model.exists():
                    p_replay_buffer = self.model_abs_dir.joinpath('save_replay_buffer')
                    if p_replay_buffer.exists():
                        self.ma_manager.save_model(p_replay_buffer.exists())
                        p_replay_buffer.unlink()
                    else:
                        self.ma_manager.save_model(False)

                    p_model.unlink()

                iteration += 1

        finally:
            if self.train_mode:
                self.ma_manager.save_model()
            self.env.close()

            self._logger.info('Training terminated')

    def _log_episode_summaries(self):
        for n, mgr in self.ma_manager:
            rewards = np.array([a.reward for a in mgr.agents])
            mgr.rl.write_constant_summaries([
                {'tag': 'reward/mean', 'simple_value': rewards.mean()},
                {'tag': 'reward/max', 'simple_value': rewards.max()},
                {'tag': 'reward/min', 'simple_value': rewards.min()}
            ])

    def _log_episode_info(self, iteration, iter_time):
        for n, mgr in self.ma_manager:
            global_step = format_global_step(mgr.rl.get_global_step())
            rewards = [a.reward for a in mgr.agents]
            rewards = ", ".join([f"{i:6.1f}" for i in rewards])
            max_step = max([a.steps for a in mgr.agents])
            self._logger.info(f'{n} {iteration}({global_step}), T {iter_time:.2f}s, S {max_step}, R {rewards}')
