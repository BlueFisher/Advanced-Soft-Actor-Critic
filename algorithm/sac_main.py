import importlib
import logging
import shutil
import sys
import time
from pathlib import Path

import numpy as np

import algorithm.config_helper as config_helper

from .agent import Agent, MultiAgentsManager
from .sac_base import SAC_Base
from .utils import format_global_step, gen_pre_n_actions
from .utils.enums import *


class Main(object):
    train_mode = True
    _agent_class = Agent  # For different environments

    def __init__(self, root_dir, config_dir, args):
        """
        config_path: the directory of config file
        args: command arguments generated by argparse
        """
        self._logger = logging.getLogger('sac')

        config_abs_dir = self._init_config(root_dir, config_dir, args)

        self._init_env()
        self._init_sac(config_abs_dir)

        self._run()

    def _init_config(self, root_dir, config_dir, args):
        config_abs_dir = Path(root_dir).joinpath(config_dir)
        config_abs_path = config_abs_dir.joinpath('config.yaml')
        default_config_abs_path = Path(__file__).resolve().parent.joinpath('default_config.yaml')
        # Merge default_config.yaml and custom config.yaml
        config, ma_configs = config_helper.initialize_config_from_yaml(default_config_abs_path,
                                                                       config_abs_path,
                                                                       args.config)

        # Initialize config from command line arguments
        self.train_mode = not args.run
        self.render = args.render

        self.run_in_editor = args.editor

        self.disable_sample = args.disable_sample
        self.alway_use_env_nn = args.use_env_nn
        self.device = args.device
        self.last_ckpt = args.ckpt

        if args.env_args is not None:
            config['base_config']['env_args'] = args.env_args
        if args.port is not None:
            config['base_config']['unity_args']['port'] = args.port
        if args.agents is not None:
            config['base_config']['n_agents'] = args.agents
        if args.max_iter is not None: 
            config['base_config']['max_iter'] = args.max_iter
        if args.name is not None:
            config['base_config']['name'] = args.name
        if args.nn is not None:
            config['sac_config']['nn'] = args.nn
            for ma_config in ma_configs.values():
                ma_config['sac_config']['nn'] = args.nn

        config['base_config']['name'] = config_helper.generate_base_name(config['base_config']['name'])

        # The absolute directory of a specific training
        model_abs_dir = Path(root_dir).joinpath('models',
                                                config['base_config']['env_name'],
                                                config['base_config']['name'])
        self.model_abs_dir = model_abs_dir

        if args.logger_in_file:
            config_helper.set_logger(model_abs_dir.joinpath(f'log.log'))

        if self.train_mode:
            config_helper.save_config(config, model_abs_dir, 'config.yaml')
        config_helper.display_config(config, self._logger)
        convert_config_to_enum(config['sac_config'])
        
        for n, ma_config in ma_configs.items():
            if self.train_mode:
                config_helper.save_config(ma_config, model_abs_dir, f'config_{n}.yaml')
            config_helper.display_config(ma_config, self._logger, n)
            convert_config_to_enum(ma_config['sac_config'])

        self.base_config = config['base_config']
        self.reset_config = config['reset_config']
        self.config = config
        self.ma_configs = ma_configs

        return config_abs_dir

    def _init_env(self):
        if self.base_config['env_type'] == 'UNITY':
            from algorithm.env_wrapper.unity_wrapper import UnityWrapper

            if self.run_in_editor:
                self.env = UnityWrapper(train_mode=self.train_mode,
                                        n_agents=self.base_config['n_agents'])
            else:
                self.env = UnityWrapper(train_mode=self.train_mode,
                                        file_name=self.base_config['unity_args']['build_path'][sys.platform],
                                        base_port=self.base_config['unity_args']['port'],
                                        no_graphics=self.base_config['unity_args']['no_graphics'] and not self.render,
                                        scene=self.base_config['env_name'],
                                        additional_args=self.base_config['env_args'],
                                        n_agents=self.base_config['n_agents'])

        elif self.base_config['env_type'] == 'GYM':
            from algorithm.env_wrapper.gym_wrapper import GymWrapper

            self.env = GymWrapper(train_mode=self.train_mode,
                                  env_name=self.base_config['env_name'],
                                  render=self.render,
                                  n_agents=self.base_config['n_agents'])

        elif self.base_config['env_type'] == 'DM_CONTROL':
            from algorithm.env_wrapper.dm_control_wrapper import \
                DMControlWrapper

            self.env = DMControlWrapper(train_mode=self.train_mode,
                                        env_name=self.base_config['env_name'],
                                        render=self.render,
                                        n_agents=self.base_config['n_agents'])

        elif self.base_config['env_type'] == 'TEST':
            from algorithm.env_wrapper.test_wrapper import TestWrapper

            self.env = TestWrapper(env_args=self.base_config['env_args'],
                                   n_agents=self.base_config['n_agents'])

        else:
            raise RuntimeError(f'Undefined Environment Type: {self.base_config["env_type"]}')

        ma_obs_shapes, ma_d_action_size, ma_c_action_size = self.env.init()
        self.ma_manager = MultiAgentsManager(self._agent_class,
                                             ma_obs_shapes,
                                             ma_d_action_size,
                                             ma_c_action_size,
                                             self.model_abs_dir)
        for n, mgr in self.ma_manager:
            if n not in self.ma_configs:
                self._logger.warning(f'{n} not in ma_configs')
                mgr.set_config(self.config)
            else:
                mgr.set_config(self.ma_configs[n])

        self._logger.info(f'{self.base_config["env_name"]} initialized')

    def _init_sac(self, config_abs_dir: Path):
        for n, mgr in self.ma_manager:
            # If nn models exists, load saved model, or copy a new one
            saved_nn_abs_path = mgr.model_abs_dir / 'saved_nn.py'
            if not self.alway_use_env_nn and saved_nn_abs_path.exists():
                spec = importlib.util.spec_from_file_location('nn', str(saved_nn_abs_path))
                self._logger.info(f'Loaded nn from existed {saved_nn_abs_path}')
            else:
                nn_abs_path = config_abs_dir / f'{mgr.config["sac_config"]["nn"]}.py'

                spec = importlib.util.spec_from_file_location('nn', str(nn_abs_path))
                self._logger.info(f'Loaded nn in env dir: {nn_abs_path}')
                if not self.alway_use_env_nn:
                    shutil.copyfile(nn_abs_path, saved_nn_abs_path)

            nn = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(nn)
            mgr.config['sac_config']['nn'] = nn

            mgr.set_sac(SAC_Base(obs_shapes=mgr.obs_shapes,
                                 d_action_size=mgr.d_action_size,
                                 c_action_size=mgr.c_action_size,
                                 model_abs_dir=mgr.model_abs_dir,
                                 device=self.device,
                                 ma_name=None if len(self.ma_manager) == 1 else n,
                                 train_mode=self.train_mode,
                                 last_ckpt=self.last_ckpt,

                                 nn_config=mgr.config['nn_config'],
                                 **mgr.config['sac_config'],

                                 replay_config=mgr.config['replay_config']))

    def _run(self):
        self.ma_manager.init(self.base_config['n_agents'])

        ma_obs_list = self.env.reset(reset_config=self.reset_config)
        self.ma_manager.set_obs_list(ma_obs_list)

        force_reset = False
        iteration = 0
        trained_steps = 0

        try:
            while iteration != self.base_config['max_iter']:
                if self.base_config['max_step'] != -1 and trained_steps >= self.base_config['max_step']:
                    break

                if self.base_config['reset_on_iteration'] \
                        or self.ma_manager.is_max_reached() \
                        or force_reset:
                    ma_obs_list = self.env.reset(reset_config=self.reset_config)
                    self.ma_manager.set_obs_list(ma_obs_list)
                    self.ma_manager.clear()

                    force_reset = False
                else:
                    self.ma_manager.reset()

                step = 0
                iter_time = time.time()

                while not self.ma_manager.is_done():
                    self.ma_manager.burn_in_padding()

                    ma_d_action, ma_c_action = self.ma_manager.get_ma_action(disable_sample=self.disable_sample)

                    (ma_next_obs_list,
                     ma_reward,
                     ma_local_done,
                     ma_max_reached) = self.env.step(ma_d_action, ma_c_action)

                    if ma_next_obs_list is None:
                        force_reset = True

                        self._logger.warning('Step encounters error, episode ignored')
                        continue

                    for n, mgr in self.ma_manager:
                        if step == self.base_config['max_step_each_iter']:
                            ma_local_done[n] = [True] * len(mgr.agents)
                            ma_max_reached[n] = [True] * len(mgr.agents)

                        episode_trans_list = [
                            a.add_transition(
                                obs_list=[o[i] for o in mgr['obs_list']],
                                action=mgr['action'][i],
                                reward=ma_reward[n][i],
                                local_done=ma_local_done[n][i],
                                max_reached=ma_max_reached[n][i],
                                next_obs_list=[o[i] for o in ma_next_obs_list[n]],
                                prob=mgr['prob'][i],
                                is_padding=False,
                                seq_hidden_state=mgr['seq_hidden_state'][i] if mgr.seq_encoder is not None else None,
                            ) for i, a in enumerate(mgr.agents)
                        ]

                        if self.train_mode:
                            episode_trans_list = [t for t in episode_trans_list if t is not None]
                            if len(episode_trans_list) != 0:
                                # ep_indexes, ep_padding_masks,
                                # ep_obses_list, ep_actions, ep_rewards, next_obs_list, ep_dones, ep_probs,
                                # ep_seq_hidden_states
                                for episode_trans in episode_trans_list:
                                    mgr.sac.put_episode(*episode_trans)
                            trained_steps = max(trained_steps, mgr.sac.train())

                    self.ma_manager.post_step(ma_next_obs_list, ma_local_done)

                    step += 1

                if self.train_mode:
                    self._log_episode_summaries()

                self._log_episode_info(iteration, time.time() - iter_time)

                p = self.model_abs_dir.joinpath('save_model')
                if self.train_mode and p.exists():
                    self.ma_manager.save_model()
                    p.unlink()

                iteration += 1

        finally:
            if self.train_mode:
                self.ma_manager.save_model()
            self.env.close()

            self._logger.info('Training terminated')

    def _log_episode_summaries(self):
        for n, mgr in self.ma_manager:
            rewards = np.array([a.reward for a in mgr.agents])
            mgr.sac.write_constant_summaries([
                {'tag': 'reward/mean', 'simple_value': rewards.mean()},
                {'tag': 'reward/max', 'simple_value': rewards.max()},
                {'tag': 'reward/min', 'simple_value': rewards.min()}
            ])

    def _log_episode_info(self, iteration, iter_time):
        for n, mgr in self.ma_manager:
            global_step = format_global_step(mgr.sac.get_global_step())
            rewards = [a.reward for a in mgr.agents]
            rewards = ", ".join([f"{i:6.1f}" for i in rewards])
            max_step = max([a.steps for a in mgr.agents])
            self._logger.info(f'{n} {iteration}({global_step}), T {iter_time:.2f}s, S {max_step}, R {rewards}')
