import importlib
import logging
import shutil
import sys
import time
from pathlib import Path

import numpy as np

import algorithm.config_helper as config_helper

from .agent import Agent
from .sac_base import SAC_Base
from .utils import format_global_step, gen_pre_n_actions
from .utils.enums import *


class Main(object):
    train_mode = True
    _agent_class = Agent  # For different environments

    def __init__(self, root_dir, config_dir, args):
        """
        config_path: the directory of config file
        args: command arguments generated by argparse
        """
        self._logger = logging.getLogger('sac')

        config_abs_dir = self._init_config(root_dir, config_dir, args)

        self._init_env()
        self._init_sac(config_abs_dir)

        self._run()

    def _init_config(self, root_dir, config_dir, args):
        config_abs_dir = Path(root_dir).joinpath(config_dir)
        config_abs_path = config_abs_dir.joinpath('config.yaml')
        default_config_abs_path = Path(__file__).resolve().parent.joinpath('default_config.yaml')
        # Merge default_config.yaml and custom config.yaml
        config = config_helper.initialize_config_from_yaml(default_config_abs_path,
                                                           config_abs_path,
                                                           args.config)

        # Initialize config from command line arguments
        self.train_mode = not args.run
        self.render = args.render

        self.run_in_editor = args.editor

        self.disable_sample = args.disable_sample
        self.alway_use_env_nn = args.use_env_nn
        self.device = args.device
        self.last_ckpt = args.ckpt

        if args.name is not None:
            config['base_config']['name'] = args.name
        if args.env_args is not None:
            config['base_config']['env_args'] = args.env_args
        if args.port is not None:
            config['base_config']['unity_args']['port'] = args.port

        if args.nn is not None:
            config['base_config']['nn'] = args.nn
        if args.agents is not None:
            config['base_config']['n_agents'] = args.agents
        if args.max_iter is not None:
            config['base_config']['max_iter'] = args.max_iter

        config['base_config']['name'] = config_helper.generate_base_name(config['base_config']['name'])

        # The absolute directory of a specific training
        model_abs_dir = Path(root_dir).joinpath('models',
                                                config['base_config']['env_name'],
                                                config['base_config']['name'])
        model_abs_dir.mkdir(parents=True, exist_ok=True)
        self.model_abs_dir = model_abs_dir

        if args.logger_in_file:
            config_helper.set_logger(Path(model_abs_dir).joinpath(f'log.log'))

        if self.train_mode:
            config_helper.save_config(config, model_abs_dir, 'config.yaml')

        config_helper.display_config(config, self._logger)

        convert_config_to_enum(config['sac_config'])

        self.base_config = config['base_config']
        self.reset_config = config['reset_config']
        self.model_config = config['model_config']
        self.replay_config = config['replay_config']
        self.sac_config = config['sac_config']

        return config_abs_dir

    def _init_env(self):
        if self.base_config['env_type'] == 'UNITY':
            from algorithm.env_wrapper.unity_wrapper import UnityWrapper

            if self.run_in_editor:
                self.env = UnityWrapper(train_mode=self.train_mode,
                                        n_agents=self.base_config['n_agents'])
            else:
                self.env = UnityWrapper(train_mode=self.train_mode,
                                        file_name=self.base_config['unity_args']['build_path'][sys.platform],
                                        base_port=self.base_config['unity_args']['port'],
                                        no_graphics=self.base_config['unity_args']['no_graphics'] and not self.render,
                                        scene=self.base_config['env_name'],
                                        additional_args=self.base_config['env_args'],
                                        n_agents=self.base_config['n_agents'])

        elif self.base_config['env_type'] == 'GYM':
            from algorithm.env_wrapper.gym_wrapper import GymWrapper

            self.env = GymWrapper(train_mode=self.train_mode,
                                  env_name=self.base_config['env_name'],
                                  render=self.render,
                                  n_agents=self.base_config['n_agents'])

        elif self.base_config['env_type'] == 'DM_CONTROL':
            from algorithm.env_wrapper.dm_control_wrapper import \
                DMControlWrapper

            self.env = DMControlWrapper(train_mode=self.train_mode,
                                        env_name=self.base_config['env_name'],
                                        render=self.render,
                                        n_agents=self.base_config['n_agents'])

        elif self.base_config['env_type'] == 'TEST':
            from algorithm.env_wrapper.test_wrapper import TestWrapper

            self.env = TestWrapper(env_args=self.base_config['env_args'],
                                   n_agents=self.base_config['n_agents'])

        else:
            raise RuntimeError(f'Undefined Environment Type: {self.base_config["env_type"]}')

        self.ma_obs_shapes, self.ma_d_action_size, self.ma_c_action_size = self.env.init()
        self.ma_names = list(self.ma_obs_shapes.keys())
        self.ma_action_size = {n: self.ma_d_action_size[n] + self.ma_c_action_size[n] for n in self.ma_names}

        self._logger.info(f'{self.base_config["env_name"]} initialized')

    def _init_sac(self, config_abs_dir: Path):
        # If nn models exists, load saved model, or copy a new one
        nn_model_abs_path = self.model_abs_dir.joinpath('nn_models.py')
        if not self.alway_use_env_nn and nn_model_abs_path.exists():
            spec = importlib.util.spec_from_file_location('nn', str(nn_model_abs_path))
            self._logger.info(f'Loaded nn from existed {nn_model_abs_path}')
        else:
            nn_abs_path = config_abs_dir.joinpath(f'{self.base_config["nn"]}.py')
            spec = importlib.util.spec_from_file_location('nn', str(nn_abs_path))
            self._logger.info(f'Loaded nn in env dir: {nn_abs_path}')
            if not self.alway_use_env_nn:
                shutil.copyfile(nn_abs_path, nn_model_abs_path)

        custom_nn_model = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(custom_nn_model)

        self.ma_sac = {}
        for i, n in enumerate(self.ma_names):
            model_abs_dir = self.model_abs_dir
            if len(self.ma_names) > 1:
                model_abs_dir = model_abs_dir / str(i)
            self.ma_sac[n] = SAC_Base(obs_shapes=self.ma_obs_shapes[n],
                                      d_action_size=self.ma_d_action_size[n],
                                      c_action_size=self.ma_c_action_size[n],
                                      model_abs_dir=model_abs_dir,
                                      model=custom_nn_model,
                                      model_config=self.model_config,
                                      device=self.device,
                                      train_mode=self.train_mode,
                                      last_ckpt=self.last_ckpt,

                                      replay_config=self.replay_config,

                                      **self.sac_config)

    def _run(self):
        num_agents = self.base_config['n_agents']
        ma_seq_encoder = {n: sac.seq_encoder for n, sac in self.ma_sac.items()}

        ma_obs_list = self.env.reset(reset_config=self.reset_config)

        ma_initial_pre_action = {n: sac.get_initial_action(num_agents) for n, sac in self.ma_sac.items()}  # [n_agents, action_size]
        ma_pre_action = ma_initial_pre_action
        ma_initial_seq_hidden_state = {}
        ma_seq_hidden_state = {}
        for n, sac in self.ma_sac.items():
            if ma_seq_encoder[n] is not None:
                ma_initial_seq_hidden_state[n] = sac.get_initial_seq_hidden_state(num_agents)  # [n_agents, *seq_hidden_state_shape]
                ma_seq_hidden_state[n] = ma_initial_seq_hidden_state[n]

        ma_agents = {n: [self._agent_class(i, self.ma_obs_shapes[n], self.ma_action_size[n],
                                           seq_hidden_state_shape=self.ma_sac[n].seq_hidden_state_shape
                                           if ma_seq_encoder[n] is not None else None)
                         for i in range(num_agents)]
                     for n in self.ma_names}

        force_reset = False
        iteration = 0
        trained_steps = 0

        try:
            while iteration != self.base_config['max_iter']:
                if self.base_config['max_step'] != -1 and trained_steps >= self.base_config['max_step']:
                    break

                if self.base_config['reset_on_iteration'] \
                        or any([any([a.max_reached for a in agents]) for agents in ma_agents.values()]) \
                        or force_reset:
                    ma_obs_list = self.env.reset(reset_config=self.reset_config)
                    for agents in ma_agents.values():
                        for agent in agents:
                            agent.clear()

                    force_reset = False
                else:
                    for agents in ma_agents.values():
                        for agent in agents:
                            agent.reset()

                step = 0
                iter_time = time.time()

                while not all([all([a.done for a in agents]) for agents in ma_agents.values()]):
                    ma_action = {}
                    ma_prob = {}
                    ma_next_seq_hidden_state = {}

                    for n, agents in ma_agents.items():
                        # burn-in padding
                        for agent in [a for a in agents if a.is_empty()]:
                            for _ in range(self.ma_sac[n].burn_in_step):
                                agent.add_transition(
                                    obs_list=[np.zeros(t, dtype=np.float32) for t in self.ma_obs_shapes[n]],
                                    action=ma_initial_pre_action[n][0],
                                    reward=0.,
                                    local_done=False,
                                    max_reached=False,
                                    next_obs_list=[np.zeros(t, dtype=np.float32) for t in self.ma_obs_shapes[n]],
                                    prob=0.,
                                    is_padding=True,
                                    seq_hidden_state=ma_initial_seq_hidden_state[n][0]
                                )

                        if ma_seq_encoder[n] == SEQ_ENCODER.RNN:
                            action, prob, next_seq_hidden_state = self.ma_sac[n].choose_rnn_action(ma_obs_list[n],
                                                                                                   ma_pre_action[n],
                                                                                                   ma_seq_hidden_state[n],
                                                                                                   disable_sample=self.disable_sample)

                        elif ma_seq_encoder[n] == SEQ_ENCODER.ATTN:
                            ep_length = min(512, max([a.episode_length for a in agents]))

                            all_episode_trans = [a.get_episode_trans(ep_length) for a in agents]
                            (all_ep_indexes,
                             all_ep_padding_masks,
                             all_ep_obses_list,
                             all_ep_actions,
                             all_all_ep_rewards,
                             all_next_obs_list,
                             all_ep_dones,
                             all_ep_probs,
                             all_ep_attn_states) = zip(*all_episode_trans)

                            ep_indexes = np.concatenate(all_ep_indexes)
                            ep_padding_masks = np.concatenate(all_ep_padding_masks)
                            ep_obses_list = [np.concatenate(o) for o in zip(*all_ep_obses_list)]
                            ep_actions = np.concatenate(all_ep_actions)
                            ep_attn_states = np.concatenate(all_ep_attn_states)

                            ep_indexes = np.concatenate([ep_indexes, ep_indexes[:, -1:] + 1], axis=1)
                            ep_padding_masks = np.concatenate([ep_padding_masks,
                                                               np.zeros_like(ep_padding_masks[:, -1:], dtype=bool)], axis=1)
                            ep_obses_list = [np.concatenate([o, np.expand_dims(t_o, 1)], axis=1)
                                             for o, t_o in zip(ep_obses_list, ma_obs_list[n])]
                            ep_pre_actions = gen_pre_n_actions(ep_actions, True)

                            action, prob, next_seq_hidden_state = self.ma_sac[n].choose_attn_action(ep_indexes,
                                                                                                    ep_padding_masks,
                                                                                                    ep_obses_list,
                                                                                                    ep_pre_actions,
                                                                                                    ep_attn_states,
                                                                                                    disable_sample=self.disable_sample)
                        else:
                            action, prob = self.ma_sac[n].choose_action(ma_obs_list[n],
                                                                        disable_sample=self.disable_sample)
                            next_seq_hidden_state = None

                        ma_action[n] = action
                        ma_prob[n] = prob
                        ma_next_seq_hidden_state[n] = next_seq_hidden_state

                    (ma_next_obs_list,
                     ma_reward,
                     ma_local_done,
                     ma_max_reached) = self.env.step({n: action[..., :self.ma_d_action_size[n]] for n, action in ma_action.items()},
                                                     {n: action[..., self.ma_d_action_size[n]:] for n, action in ma_action.items()})

                    if ma_next_obs_list is None:
                        force_reset = True

                        self._logger.warning('Step encounters error, episode ignored')
                        continue

                    for n, agents in ma_agents.items():
                        if step == self.base_config['max_step_each_iter']:
                            ma_local_done[n] = [True] * len(agents)
                            ma_max_reached[n] = [True] * len(agents)

                        episode_trans_list = [
                            agents[i].add_transition(
                                obs_list=[o[i] for o in ma_obs_list[n]],
                                action=ma_action[n][i],
                                reward=ma_reward[n][i],
                                local_done=ma_local_done[n][i],
                                max_reached=ma_max_reached[n][i],
                                next_obs_list=[o[i] for o in ma_next_obs_list[n]],
                                prob=ma_prob[n][i],
                                is_padding=False,
                                seq_hidden_state=ma_seq_hidden_state[n][i] if ma_seq_encoder[n] is not None else None,
                            ) for i in range(len(agents))
                        ]

                        if self.train_mode:
                            episode_trans_list = [t for t in episode_trans_list if t is not None]
                            if len(episode_trans_list) != 0:
                                # ep_indexes, ep_padding_masks,
                                # ep_obses_list, ep_actions, ep_rewards, next_obs_list, ep_dones, ep_probs,
                                # ep_seq_hidden_states
                                for episode_trans in episode_trans_list:
                                    self.ma_sac[n].put_episode(*episode_trans)
                            trained_steps = self.ma_sac[n].train()

                        ma_obs_list[n] = ma_next_obs_list[n]
                        ma_pre_action[n] = ma_action[n]
                        ma_pre_action[n][ma_local_done[n]] = ma_initial_pre_action[n][ma_local_done[n]]
                        if ma_seq_encoder[n] is not None:
                            ma_seq_hidden_state[n] = ma_next_seq_hidden_state[n]
                            ma_seq_hidden_state[n][ma_local_done[n]] = ma_initial_seq_hidden_state[n][ma_local_done[n]]

                    step += 1

                if self.train_mode:
                    self._log_episode_summaries(ma_agents)

                self._log_episode_info(iteration, time.time() - iter_time, ma_agents)

                p = self.model_abs_dir.joinpath('save_model')
                if self.train_mode and p.exists():
                    self.sac.save_model()
                    p.unlink()

                iteration += 1

        finally:
            if self.train_mode:
                for sac in self.ma_sac.values():
                    sac.save_model()
            self.env.close()

            self._logger.info('Training terminated')

    def _log_episode_summaries(self, ma_agents):
        for n, agents in ma_agents.items():
            rewards = np.array([a.reward for a in agents])
            self.ma_sac[n].write_constant_summaries([
                {'tag': 'reward/mean', 'simple_value': rewards.mean()},
                {'tag': 'reward/max', 'simple_value': rewards.max()},
                {'tag': 'reward/min', 'simple_value': rewards.min()}
            ])

    def _log_episode_info(self, iteration, iter_time, ma_agents):
        for n, agents in ma_agents.items():
            global_step = format_global_step(self.ma_sac[n].get_global_step())
            rewards = [a.reward for a in agents]
            rewards = ", ".join([f"{i:6.1f}" for i in rewards])
            max_step = max([a.steps for a in agents])
            self._logger.info(f'{n} {iteration}({global_step}), T {iter_time:.2f}s, S {max_step}, R {rewards}')
