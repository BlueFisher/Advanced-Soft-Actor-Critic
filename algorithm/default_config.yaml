base_config:
  env_type: UNITY # UNITY or GYM
  scene: scene # The scene name. 
               # If in Unity envs, it indicates the specific scene. 
               # If in Gym envs, it is just a readable name displayed in TensorBoard

  # Only for Unity Environments
  build_path: # Unity executable path
    win32: path_win32
    linux: path_linux
  port: 5005
  
  # Only for Gym Enviroments
  build_path: GymEnv # Like CartPole-v1

  name: "{time}" # Training name. Placeholder "{time}" will be replaced to the time that trianing begins
  sac: sac # Neural network models file
  n_agents: 1 # N agents running in parallel
  max_iter: 1000 # Max iteration
  max_step: -1 # Max step in each iteration
  save_model_per_iter: 500 # Save model parameters every N iterations
  reset_on_iteration: true # If to force reset agent if an episode terminated

reset_config: null # Reset parameters sent to Unity

replay_config:
  batch_size: 256
  capacity: 1000000
  alpha: 0.9 # PER: [0~1] convert the importance of TD error to priority
  beta: 0.4 # PER: Importance-sampling, from initial value increasing to 1
  beta_increment_per_sampling: 0.001
  td_error_min: 0.01 # Small amount to avoid zero priority
  td_error_max: 1. # Clipped abs error
  use_mongodb: false # TODO

sac_config:
  seed: null # Random seed
  write_summary_per_step: 20 # Write summaries in TensorBoard every N steps

  burn_in_step: 0 # Burn-in steps in R2D2
  n_step: 1 # Update Q function by N steps
  use_rnn: false # If use RNN

  tau: 0.005 # Coefficient of updating target network
  update_target_per_step: 1 # Update target network every N steps
  init_log_alpha: -2.3 # The initial log_alpha
  use_auto_alpha: true # If use automating entropy adjustment
  rep_lr: 0.0003 # Learning rate for representation model
  q_lr: 0.0003 # Learning rate for Q
  policy_lr: 0.0003 # Learning rate for policy
  alpha_lr: 0.0003 # Learning rate for alpha
  gamma: 0.99 # Discount factor
  _lambda: 1.0 # Discount factor for V-trace
  use_priority: true # If use PER
  use_n_step_is: true # If use importance sampling
  use_prediction: false # If train a transition model
  use_reward_normalization: false # If use reward normalization
  use_curiosity: false # If use curiosity
  curiosity_strength: 1 # Curiosity strength if use curiosity
